{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115502d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce9e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b820358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=u7ye0GHmlyMUCz0gp4x5TtbVUgVqts&access_type=offline&code_challenge=ulXsmBB0wa46PvoxqnW4FtbW5fv6OrxkImND7P7pw4Q&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\Vadim\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sunny-lightning-392907\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\Vadim\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sunny-lightning-392907\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login\n",
    "!gcloud auth application-default set-quota-project sunny-lightning-392907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f763012",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are an assistant who helps users understand more about our application. You need to answer in one or two sentences.\n",
    "Users will ask you questions and you will answer them given the below information. Don't answer in more than three sentences:\n",
    "\n",
    "About the application:\n",
    "Our application is an outlook plugin which helps users to analyze and write their emails more efficiently. it has the following functions:\n",
    "1. It can show a sentiment of email, i.e. negative or positive. This will help users to avoid sending negative emails and try to write more positive emails\n",
    "2. It can provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language\n",
    "3. It can summarize long emails in a few sentences. This can save time for users and avoid reading long email chains\n",
    "4. It can rewrite an email in more polite and positive language.\n",
    "5. It can translate to other languages\n",
    "\n",
    "User question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca7c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudpickle is not installed. Please call `pip install google-cloud-aiplatform[preview]`.\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import vertexai\n",
    "\n",
    "def send_user_input(project_id, location, question, temperature: float = .1):\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    \n",
    "    parameters = {\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": 64,\n",
    "        \"top_p\": .8,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    request_text = context + question \n",
    "    #print(request_text)\n",
    "    response = model.predict(request_text, **parameters)\n",
    "    #print(f\"Response from Model: {response.text}\")\n",
    "    return response.text\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28befb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Answers/HackathonQuestionsExcel.xlsx\",header=0,converters={'id':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11dca037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What's the name of the Banking Unicorn?\",\n",
       " 'How does the Banking Unicorn work?',\n",
       " 'Can the Banking Unicorn learn and improve its responses over time?',\n",
       " 'Is the Banking Unicorn based on a specific AI technology or framework?',\n",
       " \"What's the purpose of having an Banking Unicorn at your exhibition stand?\",\n",
       " 'Can the Banking Unicorn interact with other AI technologies or devices?',\n",
       " 'Is the Banking Unicorn customizable in any way?',\n",
       " \"What are the limitations of the Banking Unicorn's capabilities?\",\n",
       " 'Can I take a picture with the Banking Unicorn?',\n",
       " 'What is MailboxIQ and what does it do?',\n",
       " 'What inspired you to develop the MailboxIQ App?',\n",
       " 'How can the MailboxIQ App help individuals or businesses?',\n",
       " 'Is the MailboxIQ App available for download or use?',\n",
       " 'What platforms or devices is the MailboxIQ App compatible with?',\n",
       " 'Can you provide some key features or benefits of the MailboxIQ App?',\n",
       " 'Are there any success stories or use cases for the MailboxIQ App?',\n",
       " 'How do you ensure the security and privacy of user data in the app?',\n",
       " 'Can you explain the technology behind the MailboxIQ App in simple terms?',\n",
       " 'What future developments or updates can users expect from the app?',\n",
       " 'How can someone get started with using the MailboxIQ App?',\n",
       " 'Can you tell me more about your brand and its connection to the Banking Unicorn?',\n",
       " 'Is there a specific goal or message you aim to convey through the Banking Unicorn?',\n",
       " 'How can attendees interact with the Banking Unicorn at the exhibition stand?',\n",
       " 'Are there any prizes or incentives for engaging with the Banking Unicorn?',\n",
       " 'Can the Banking Unicorn provide information about your team and company?',\n",
       " 'Can you please tell me about your team?',\n",
       " 'Default']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df.iloc[:, 1].values)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f835ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "embeddings = model.encode(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb419fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94933844, 9)\n"
     ]
    }
   ],
   "source": [
    "def get_score(model, test_sentence, questions):\n",
    "    test_emb = model.encode(test_sentence)\n",
    "    test_emb = test_emb.reshape(1, -1)\n",
    "    max_cs = 0\n",
    "    max_idx = -1\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        cs = cosine_similarity(test_emb, emb.reshape(1, -1))[0][0]\n",
    "        if cs > max_cs:\n",
    "            max_cs = cs\n",
    "            max_idx = i    \n",
    "    return (max_cs, max_idx)\n",
    "print(get_score(model, \"What's MailboxIQ?\", questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd50cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_voice_file=\"response.mp3\"\n",
    "output_voice_file_changed = \"response_changed.mp3\"\n",
    "idle_video_file = \"assets/idle9s.mp4\"\n",
    "#special videos:\n",
    "special_videos = [ (\"assets/hello.mp4\", \"assets/hello.mp3\"), \n",
    "                  (\"assets/you_can_ask_me.mp4\", \"assets/you_can_ask_me.mp3\")\n",
    "                 ]\n",
    "hello_video_file = \"assets/hello.mp4\"\n",
    "hello_audio_file = \"assets/hello.mp3\"\n",
    "#other videos\n",
    "listen_video_file = \"assets/listen_long.mp4\"\n",
    "speaking_video_file = \"assets/speaking.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6603ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vadim's Laptop\n",
    "screen_size = [1920,1080]\n",
    "micro_pos = [1300,710]\n",
    "space_key_text_pos = [450,800]\n",
    "speak_text_pos = [800,800]\n",
    "ask_me_text_pos = [1370, 200]\n",
    "text_question_pos = [450,900]\n",
    "text_question_color = (255,255,0)\n",
    "text_response_pos = [450,1000]\n",
    "text_response_color = (255,0,0)\n",
    "text_reponse_shift_speed = 7\n",
    "\n",
    "sample_questions_img_pos = [1360, 180]\n",
    "video_scale = 1.0\n",
    "\n",
    "# predefined questions, similarity threshold\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "# parameters\n",
    "voice_shift = 7\n",
    "adjust_for_ambient_noise_duration = 0.2\n",
    "log_file = 'conversation.log'\n",
    "mic_device_index = 1 # sr.Microphone.list_microphone_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d3687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import cv2\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from threading import Thread, Event\n",
    "from enum import Enum\n",
    "from pygame import mixer\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import shutil\n",
    "from random import random, randint\n",
    "\n",
    "def log(*msg):\n",
    "    print(*msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        for m in msg:\n",
    "            f.write(str(m))\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    \n",
    "class VoiceRecordingThread(Thread):\n",
    "    def run(self):\n",
    "        self.response = \"\"\n",
    "        self.thinking = False\n",
    "        r = sr.Recognizer()\n",
    "        self.recognizer = r\n",
    "        with sr.Microphone(device_index=mic_device_index) as source:\n",
    "            r.adjust_for_ambient_noise(source, duration=adjust_for_ambient_noise_duration)\n",
    "            log(\"speak\")\n",
    "            audio = r.listen(source)\n",
    "        user_input = r.recognize_google(audio)\n",
    "        self.question = user_input\n",
    "        log(\"User input:\", user_input)\n",
    "        score, idx = get_score(model, user_input, questions)\n",
    "        log(\"scores:\", score, idx)\n",
    "        if score > similarity_threshold:\n",
    "            log(\"use predefined question: \", questions[idx])\n",
    "            file_id = df.iloc[idx, 0]\n",
    "            shutil.copyfile(\"Answers/\"+file_id+\"_response_unicorn.mp3\", output_voice_file_changed)\n",
    "            self.response = df.iloc[idx, 2]\n",
    "        else:\n",
    "            self.thinking = True\n",
    "            self.response = send_user_input('sunny-lightning-392907', 'us-central1', user_input)\n",
    "            log(\"Model response:\", self.response)\n",
    "            voice_prep(self.response)\n",
    "            self.thinking = False\n",
    "     \n",
    "    \n",
    "class PlaySoundThread(Thread):\n",
    "    def __init__(self, stop_event):\n",
    "        Thread.__init__(self)\n",
    "        self.stop_event = stop_event\n",
    "        \n",
    "    def run(self):\n",
    "        mixer.init()\n",
    "        mixer.music.load(output_voice_file_changed)\n",
    "        mixer.music.play()\n",
    "        while(mixer.music.get_busy() and not self.stop_event.is_set()):\n",
    "            pass\n",
    "        mixer.music.unload()\n",
    "        log(\"audio end\")\n",
    "\n",
    "        \n",
    "        \n",
    "def voice_prep(response):\n",
    "    language = 'en'\n",
    "\n",
    "    speech = gTTS(text=response, lang=language)\n",
    "\n",
    "    speech.save(output_voice_file)\n",
    "    save_shifted_pitch(output_voice_file, output_voice_file_changed, voice_shift)\n",
    "    #save_shifted_pitch(\"response\", 7, )\n",
    "    #os.system(\"start response_shifted.wav\")\n",
    "    #os.system(\"start \" + output_voice_file)\n",
    "    #playsound(output_voice_file)\n",
    "\n",
    "def printText(frame, text, pos = 50):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (pos, 1000)\n",
    "    fontScale = 2\n",
    "    color = (0, 255, 255)\n",
    "    thickness = 4\n",
    "    frame = cv2.putText(frame, text, org, font, \n",
    "                       fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "    \n",
    "class States(Enum):\n",
    "    IDLE = 1\n",
    "    LISTENING = 2\n",
    "    SPEAKING = 3\n",
    "\n",
    "def unicornVideo():\n",
    "    pygame.init()\n",
    "\n",
    "    # prepare assets\n",
    "\n",
    "    micro_img_white = pygame.image.load('assets/micro_white.png')\n",
    "    micro_img_black = pygame.image.load('assets/micro.png')\n",
    "    micro_img = micro_img_black\n",
    "\n",
    "    big_font = pygame.font.SysFont('comicsansms', 48)\n",
    "    space_key_text = big_font.render('Press and hold Space key to speak', True, (0,0,0))\n",
    "    speak_text = big_font.render('please speak', True, (0,0,0))\n",
    "    thinking_text = big_font.render('thinking..', True, (0,0,0))\n",
    "\n",
    "    small_font = pygame.font.SysFont('comicsansms', 24)\n",
    "    ask_me_text = small_font.render('What can you ask me?', True, (255,255,255))\n",
    "    sample_questions_img = pygame.image.load('assets/sample_questions.png')\n",
    "\n",
    "    # current video\n",
    "\n",
    "    cap = cv2.VideoCapture(idle_video_file)\n",
    "\n",
    "    #pygame.display.set_caption(\"OpenCV camera stream on Pygame\")\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "    shape = None\n",
    "    \n",
    "    #\n",
    "    \n",
    "    speaking_thread = None\n",
    "    listening_thread = None\n",
    "    state = States.IDLE\n",
    "    text_response = None\n",
    "    text_reponse_shift = 0\n",
    "    text_question = None   \n",
    "    hello_played = False\n",
    "    stop_event = None\n",
    "    try:\n",
    "        while(cap.isOpened()):\n",
    "            if state == States.LISTENING:\n",
    "                if not listening_thread.is_alive():\n",
    "                    #print(\"thread not alive\")\n",
    "                    text_response = listening_thread.response\n",
    "                    text_question = listening_thread.question\n",
    "                    text_reponse_shift = 0\n",
    "                    listening_thread = None\n",
    "                    state = States.SPEAKING\n",
    "                    stop_event = Event()\n",
    "                    speaking_thread = PlaySoundThread(stop_event)\n",
    "                    if text_response:\n",
    "                        speaking_thread.start()\n",
    "                        cap.release()\n",
    "                        cap = cv2.VideoCapture(speaking_video_file)\n",
    "            elif state == States.SPEAKING:\n",
    "                if not speaking_thread.is_alive():\n",
    "                    speaking_thread = None\n",
    "                    state = States.IDLE\n",
    "                    cap.release()\n",
    "                    cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                screen.fill([0,0,0])\n",
    "                shape = (frame.shape[0]*video_scale, frame.shape[1]*video_scale)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = np.rot90(frame)\n",
    "                frame = pygame.surfarray.make_surface(frame)\n",
    "                if video_scale != 1:\n",
    "                    frame = pygame.transform.scale_by(frame, video_scale)\n",
    "\n",
    "                if screen_size[0] > shape[0]:\n",
    "                    screen.blit(frame, ( (screen_size[0] - shape[0])/2,0))\n",
    "                else:\n",
    "                    screen.blit(frame, (0,0))\n",
    "\n",
    "                #display UI\n",
    "                screen.blit(micro_img, micro_pos)\n",
    "                screen.blit(sample_questions_img, sample_questions_img_pos)\n",
    "\n",
    "                # display text\n",
    "                if state == States.IDLE:\n",
    "                    screen.blit(space_key_text, space_key_text_pos)\n",
    "                elif state == States.LISTENING:\n",
    "                    if listening_thread and listening_thread.thinking:\n",
    "                        screen.blit(thinking_text, speak_text_pos)\n",
    "                    else:\n",
    "                        screen.blit(speak_text, speak_text_pos)\n",
    "                elif state == States.SPEAKING:\n",
    "                    if text_question:\n",
    "                        screen.blit(small_font.render(\"Your question:\" +text_question, True, text_question_color), text_question_pos)\n",
    "                    if text_response:\n",
    "                        text_reponse_shift += text_reponse_shift_speed\n",
    "                        screen.blit(small_font.render(text_response, True, text_response_color),\n",
    "                                    [text_response_pos[0]-text_reponse_shift, text_response_pos[1]])\n",
    "\n",
    "                \n",
    "                \n",
    "                # update\n",
    "                pygame.display.update()\n",
    "\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == KEYDOWN:\n",
    "                        if event.key == pygame.K_SPACE and state == States.IDLE:\n",
    "                            micro_img = micro_img_white\n",
    "                            state = States.LISTENING\n",
    "                            log(\"space\")\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(listen_video_file)\n",
    "\n",
    "                            listening_thread = VoiceRecordingThread()\n",
    "                            listening_thread.start()\n",
    "                            log(\"listening thread is started\")\n",
    "\n",
    "                        elif event.key == pygame.K_q:\n",
    "                            # exit\n",
    "                            sys.exit(0)\n",
    "                        elif event.key == pygame.K_b:\n",
    "                            # break and go to idle cycle\n",
    "                            if stop_event:\n",
    "                                stop_event.set()\n",
    "                            speaking_thread = None\n",
    "                            listening_thread = None\n",
    "                            state = States.IDLE\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(idle_video_file)\n",
    "                            #sys.exit(0)\n",
    "                    elif event.type == KEYUP:\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            micro_img = micro_img_black\n",
    "                            if listening_thread:\n",
    "                                listening_thread.recognizer.energy_threshold = 10000\n",
    "                    elif event.type == pygame.QUIT:\n",
    "                        sys.exit(0)\n",
    "\n",
    "                cv2.waitKey(25)\n",
    "                \n",
    "            else:\n",
    "                if state == States.IDLE:\n",
    "                    # random play hello\n",
    "                    if state == States.IDLE:\n",
    "                        if random() < 0.5 and not hello_played:\n",
    "                            hello_played = True\n",
    "                            rand_video_id = randint(0, len(special_videos)-1)\n",
    "                            cap = cv2.VideoCapture(special_videos[rand_video_id][0])\n",
    "                            mixer.init()\n",
    "                            mixer.music.load(special_videos[rand_video_id][1])\n",
    "                            mixer.music.play()\n",
    "                            \n",
    "                           # mixer.music.unload()\n",
    "                        else:\n",
    "                            hello_played = False\n",
    "                            cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "                elif state == States.LISTENING:\n",
    "                    cap = cv2.VideoCapture(listen_video_file)\n",
    "                else:\n",
    "                    cap = cv2.VideoCapture(speaking_video_file)\n",
    "                    \n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        pygame.quit()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "\n",
    "def shift_pitch(filename, higher_pitch_shift, lower_pitch_shift, load_original=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(filename)\n",
    "    if load_original:\n",
    "        IPython.display.display(Audio(data=audio_wavelength, rate=sampling_rate, autoplay=False))\n",
    "    if higher_pitch_shift > 0:\n",
    "        higher_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=higher_pitch_shift)\n",
    "        IPython.display.display(Audio(data=higher_pitch, rate=sampling_rate, autoplay=False))\n",
    "    if lower_pitch_shift < 0:\n",
    "        lower_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=lower_pitch_shift)\n",
    "        IPython.display.display(Audio(data=lower_pitch, rate=sampling_rate, autoplay=False))\n",
    "        \n",
    "def save_shifted_pitch(output_voice_file, output_voice_file_changed, pitch_shift, load_original=False, load_altered=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(output_voice_file)\n",
    "    altered = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=pitch_shift)\n",
    "    sf.write(output_voice_file_changed, altered, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb7cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: can you tell me about your plugin please\n",
      "scores: 0.36708522 14\n",
      "Model response: Our plugin is an Outlook plugin that helps users to analyze and write their emails more efficiently. It can show a sentiment of email, i.e. negative or positive. It can also provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language.\n",
      "audio end\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unicornVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d55e916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.energy_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca8c4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_prep(\"you can ask me questions about Mailbox IQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da302939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio end\n"
     ]
    }
   ],
   "source": [
    "PlaySoundThread(Event()).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa0c5e",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
