{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115502d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b820358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=5KJSnXAc8qntt42t92IvKp9KTOftfX&access_type=offline&code_challenge=R-YlGTNXUK8ZCNX7QHrgzYbWdbFycf-nyzNwjBjmIIs&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\Vadim\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sunny-lightning-392907\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\Vadim\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sunny-lightning-392907\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login\n",
    "!gcloud auth application-default set-quota-project sunny-lightning-392907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f763012",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are an assistant who helps users understand more about our application. You need to answer in one or two sentences.\n",
    "Users will ask you questions and you will answer them given the below information. Don't answer in more than three sentences:\n",
    "\n",
    "About the application:\n",
    "Our application is an outlook plugin which helps users to analyze and write their emails more efficiently. it has the following functions:\n",
    "1. It can show a sentiment of email, i.e. negative or positive. This will help users to avoid sending negative emails and try to write more positive emails\n",
    "2. It can provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language\n",
    "3. It can summarize long emails in a few sentences. This can save time for users and avoid reading long email chains\n",
    "4. It can rewrite an email in more polite and positive language.\n",
    "5. It can translate to other languages\n",
    "\n",
    "User question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca7c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudpickle is not installed. Please call `pip install google-cloud-aiplatform[preview]`.\n"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import vertexai\n",
    "\n",
    "def send_user_input(project_id, location, question, temperature: float = .1):\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    \n",
    "    parameters = {\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": 64,\n",
    "        \"top_p\": .8,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    request_text = context + question \n",
    "    #print(request_text)\n",
    "    response = model.predict(request_text, **parameters)\n",
    "    #print(f\"Response from Model: {response.text}\")\n",
    "    return response.text\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28befb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Answers/HackathonQuestionsExcel.xlsx\",header=0,converters={'id':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dca037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What's the name of the Banking Unicorn?\",\n",
       " 'How does the Banking Unicorn work?',\n",
       " 'Can the Banking Unicorn learn and improve its responses over time?',\n",
       " 'Is the Banking Unicorn based on a specific AI technology or framework?',\n",
       " \"What's the purpose of having an Banking Unicorn at your exhibition stand?\",\n",
       " 'Can the Banking Unicorn interact with other AI technologies or devices?',\n",
       " 'Is the Banking Unicorn customizable in any way?',\n",
       " \"What are the limitations of the Banking Unicorn's capabilities?\",\n",
       " 'Can I take a picture with the Banking Unicorn?',\n",
       " 'What is MailboxIQ and what does it do?',\n",
       " 'What inspired you to develop the MailboxIQ App?',\n",
       " 'How can the MailboxIQ App help individuals or businesses?',\n",
       " 'Is the MailboxIQ App available for download or use?',\n",
       " 'What platforms or devices is the MailboxIQ App compatible with?',\n",
       " 'Can you provide some key features or benefits of the MailboxIQ App?',\n",
       " 'Are there any success stories or use cases for the MailboxIQ App?',\n",
       " 'How do you ensure the security and privacy of user data in the app?',\n",
       " 'Can you explain the technology behind the MailboxIQ App in simple terms?',\n",
       " 'What future developments or updates can users expect from the app?',\n",
       " 'How can someone get started with using the MailboxIQ App?',\n",
       " 'Can you tell me more about your brand and its connection to the Banking Unicorn?',\n",
       " 'Is there a specific goal or message you aim to convey through the Banking Unicorn?',\n",
       " 'How can attendees interact with the Banking Unicorn at the exhibition stand?',\n",
       " 'Are there any prizes or incentives for engaging with the Banking Unicorn?',\n",
       " 'Can the Banking Unicorn provide information about your team and company?',\n",
       " 'Can you please tell me about your team?',\n",
       " 'Default']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df.iloc[:, 1].values)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f835ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "embeddings = model.encode(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb419fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94933844, 9)\n"
     ]
    }
   ],
   "source": [
    "def get_score(model, test_sentence, questions):\n",
    "    test_emb = model.encode(test_sentence)\n",
    "    test_emb = test_emb.reshape(1, -1)\n",
    "    max_cs = 0\n",
    "    max_idx = -1\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        cs = cosine_similarity(test_emb, emb.reshape(1, -1))[0][0]\n",
    "        if cs > max_cs:\n",
    "            max_cs = cs\n",
    "            max_idx = i    \n",
    "    return (max_cs, max_idx)\n",
    "print(get_score(model, \"What's MailboxIQ?\", questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd50cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_voice_file=\"response_new2.mp3\"\n",
    "output_voice_file_changed = \"response_new2_changed.mp3\"\n",
    "idle_video_file = \"idle9s.mp4\"\n",
    "#input_video_file = \"listen.mp4\"\n",
    "listen_video_file = \"listen_long.mp4\"\n",
    "speaking_video_file = \"video_assistant.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6603ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vadim's Laptop\n",
    "screen_size = [1920,1080]\n",
    "micro_pos = [1300,710]\n",
    "space_key_text_pos = [450,800]\n",
    "speak_text_pos = [800,800]\n",
    "ask_me_text_pos = [1370, 200]\n",
    "\n",
    "sample_questions_img_pos = [1360, 180]\n",
    "video_scale = 1.0\n",
    "\n",
    "# predefined questions, similarity threshold\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "# parameters\n",
    "voice_shift = 7\n",
    "adjust_for_ambient_noise_duration = 0.2\n",
    "log_file = 'conversation.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70d3687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import cv2\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from threading import Thread\n",
    "from enum import Enum\n",
    "from pygame import mixer\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import shutil\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(msg+'\\n')\n",
    "    \n",
    "class VoiceRecordingThread(Thread):\n",
    "    def run(self):\n",
    "        self.response = \"\"\n",
    "        r = sr.Recognizer()\n",
    "        with sr.Microphone(device_index=0) as source:\n",
    "            r.adjust_for_ambient_noise(source, duration=adjust_for_ambient_noise_duration)\n",
    "            #frame = printText(frame, \"speak\")\n",
    "            #cv2.imshow('Unicorn', frame)\n",
    "            #quitButton = cv2.waitKey(25) & 0xFF == ord('q')\n",
    "            log(\"speak\")\n",
    "            audio = r.listen(source)\n",
    "        user_input = r.recognize_google(audio)\n",
    "        log(\"User input:\", user_input)\n",
    "        score, idx = get_score(model, user_input, questions)\n",
    "        log(\"scores:\", score, idx)\n",
    "        if score > similarity_threshold:\n",
    "            log(\"use predefined question: \", questions[idx])\n",
    "            file_id = df.iloc[idx, 0]\n",
    "            shutil.copyfile(\"Answers/\"+file_id+\"_response_unicorn.mp3\", output_voice_file_changed)\n",
    "            self.response = df.iloc[idx, 2]\n",
    "        else:\n",
    "            self.response = send_user_input('sunny-lightning-392907', 'us-central1', user_input)\n",
    "            log(\"Model response:\", self.response)\n",
    "            voice_prep(self.response)\n",
    "     \n",
    "    \n",
    "class PlaySoundThread(Thread):\n",
    "    def run(self):\n",
    "        mixer.init()\n",
    "        mixer.music.load(output_voice_file_changed)\n",
    "        mixer.music.play()\n",
    "        while(mixer.music.get_busy()):\n",
    "            pass\n",
    "        mixer.music.unload()\n",
    "        log(\"audio end\")\n",
    "\n",
    "        \n",
    "        \n",
    "def voice_prep(response):\n",
    "    language = 'en'\n",
    "\n",
    "    speech = gTTS(text=response, lang=language)\n",
    "\n",
    "    speech.save(output_voice_file)\n",
    "    save_shifted_pitch(output_voice_file, output_voice_file_changed, voice_shift)\n",
    "    #save_shifted_pitch(\"response\", 7, )\n",
    "    #os.system(\"start response_shifted.wav\")\n",
    "    #os.system(\"start \" + output_voice_file)\n",
    "    #playsound(output_voice_file)\n",
    "\n",
    "def printText(frame, text, pos = 50):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (pos, 1000)\n",
    "    fontScale = 2\n",
    "    color = (0, 255, 255)\n",
    "    thickness = 4\n",
    "    frame = cv2.putText(frame, text, org, font, \n",
    "                       fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "    \n",
    "class States(Enum):\n",
    "    IDLE = 1\n",
    "    LISTENING = 2\n",
    "    SPEAKING = 3\n",
    "\n",
    "def unicornVideo():\n",
    "    pygame.init()\n",
    "\n",
    "    # prepare assets\n",
    "\n",
    "    micro_img_white = pygame.image.load('micro_white.png')\n",
    "    micro_img_black = pygame.image.load('micro.png')\n",
    "    micro_img = micro_img_black\n",
    "\n",
    "    big_font = pygame.font.SysFont('comicsansms', 48)\n",
    "    space_key_text = big_font.render('Press and hold Space key to speak', True, (0,0,0))\n",
    "    speak_text = big_font.render('please speak', True, (0,0,0))\n",
    "\n",
    "    small_font = pygame.font.SysFont('comicsansms', 24)\n",
    "    ask_me_text = small_font.render('What can you ask me?', True, (255,255,255))\n",
    "    sample_questions_img = pygame.image.load('sample_questions.png')\n",
    "\n",
    "    # current video\n",
    "\n",
    "    cap = cv2.VideoCapture(idle_video_file)\n",
    "\n",
    "    #pygame.display.set_caption(\"OpenCV camera stream on Pygame\")\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "    shape = None\n",
    "    \n",
    "    #\n",
    "    \n",
    "    speaking_thread = None\n",
    "    listening_thread = None\n",
    "    state = States.IDLE\n",
    "    text_response = None   \n",
    "    \n",
    "    try:\n",
    "        while(cap.isOpened()):\n",
    "            if state == States.LISTENING:\n",
    "                if not listening_thread.is_alive():\n",
    "                    #print(\"thread not alive\")\n",
    "                    text_response = listening_thread.response\n",
    "                    listening_thread = None\n",
    "                    state = States.SPEAKING\n",
    "                    speaking_thread = PlaySoundThread()\n",
    "                    if text_response:\n",
    "                        speaking_thread.start()\n",
    "                        cap.release()\n",
    "                        cap = cv2.VideoCapture(speaking_video_file)\n",
    "            elif state == States.SPEAKING:\n",
    "                if not speaking_thread.is_alive():\n",
    "                    speaking_thread = None\n",
    "                    state = States.IDLE\n",
    "                    cap.release()\n",
    "                    cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                screen.fill([0,0,0])\n",
    "                shape = (frame.shape[0]*video_scale, frame.shape[1]*video_scale)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = np.rot90(frame)\n",
    "                frame = pygame.surfarray.make_surface(frame)\n",
    "                if video_scale != 1:\n",
    "                    frame = pygame.transform.scale_by(frame, video_scale)\n",
    "\n",
    "                if screen_size[0] > shape[0]:\n",
    "                    screen.blit(frame, ( (screen_size[0] - shape[0])/2,0))\n",
    "                else:\n",
    "                    screen.blit(frame, (0,0))\n",
    "\n",
    "                #display UI\n",
    "                screen.blit(micro_img, micro_pos)\n",
    "\n",
    "                if state == States.IDLE:\n",
    "                    screen.blit(space_key_text, space_key_text_pos)\n",
    "                elif state == States.LISTENING:\n",
    "                    screen.blit(speak_text, speak_text_pos)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                #screen.blit(ask_me_text, ask_me_text_pos)\n",
    "\n",
    "                screen.blit(sample_questions_img, sample_questions_img_pos)\n",
    "                # update\n",
    "\n",
    "                pygame.display.update()\n",
    "\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == KEYDOWN:\n",
    "                        if event.key == pygame.K_SPACE and state == States.IDLE:\n",
    "                            micro_img = micro_img_white\n",
    "                            state = States.LISTENING\n",
    "                            log(\"space\")\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(listen_video_file)\n",
    "\n",
    "                            listening_thread = VoiceRecordingThread()\n",
    "                            listening_thread.start()\n",
    "                            log(\"listening thread is started\")\n",
    "\n",
    "                        elif event.key == pygame.K_q:\n",
    "                            sys.exit(0)\n",
    "                    elif event.type == KEYUP:\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            micro_img = micro_img_black\n",
    "                    elif event.type == pygame.QUIT:\n",
    "                        sys.exit(0)\n",
    "\n",
    "                cv2.waitKey(25)\n",
    "            else:\n",
    "                if state == States.IDLE:\n",
    "                    cap = cv2.VideoCapture(idle_video_file)\n",
    "                elif state == States.LISTENING:\n",
    "                    cap = cv2.VideoCapture(listen_video_file)\n",
    "                else:\n",
    "                    cap = cv2.VideoCapture(speaking_video_file)\n",
    "                    \n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        pygame.quit()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "\n",
    "def shift_pitch(filename, higher_pitch_shift, lower_pitch_shift, load_original=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(filename)\n",
    "    if load_original:\n",
    "        IPython.display.display(Audio(data=audio_wavelength, rate=sampling_rate, autoplay=False))\n",
    "    if higher_pitch_shift > 0:\n",
    "        higher_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=higher_pitch_shift)\n",
    "        IPython.display.display(Audio(data=higher_pitch, rate=sampling_rate, autoplay=False))\n",
    "    if lower_pitch_shift < 0:\n",
    "        lower_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=lower_pitch_shift)\n",
    "        IPython.display.display(Audio(data=lower_pitch, rate=sampling_rate, autoplay=False))\n",
    "        \n",
    "def save_shifted_pitch(output_voice_file, output_voice_file_changed, pitch_shift, load_original=False, load_altered=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(output_voice_file)\n",
    "    altered = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=pitch_shift)\n",
    "    sf.write(output_voice_file_changed, altered, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0cb7cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vadim\\AppData\\Local\\Temp\\ipykernel_15380\\1272385917.py\", line 30, in run\n",
      "  File \"D:\\workspace\\dev\\conda\\envs\\assistant\\Lib\\site-packages\\speech_recognition\\__init__.py\", line 383, in adjust_for_ambient_noise\n",
      "    assert source.stream is not None, \"Audio source must be entered before adjusting, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: Audio source must be entered before adjusting, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\workspace\\dev\\conda\\envs\\assistant\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Vadim\\AppData\\Local\\Temp\\ipykernel_15380\\1272385917.py\", line 29, in run\n",
      "  File \"D:\\workspace\\dev\\conda\\envs\\assistant\\Lib\\site-packages\\speech_recognition\\__init__.py\", line 189, in __exit__\n",
      "    self.stream.close()\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'close'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space\n",
      "listening thread is started\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unicornVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43ec8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_prep(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7ad988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio end\n"
     ]
    }
   ],
   "source": [
    "PlaySoundThread().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96470be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = pygame.font.get_fonts()\n",
    "print(len(fonts))\n",
    "for f in fonts:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa0c5e",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
