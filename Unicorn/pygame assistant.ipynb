{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115502d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b820358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=5Nr3Ji4w9bNpzGDRpwUSk7eAWsSsky&access_type=offline&code_challenge=H7OKydl6xCAoJy0R_iL3ta-tys4ts7GMMNzEwpsBRUM&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\vadim\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "WARNING: \n",
      "Cannot add the project \"hack-team-fdw\" to ADC as the quota project because the account in ADC does not have the \"serviceusage.services.use\" permission on this project. You might receive a \"quota_exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n",
      "\n",
      "Credentials saved to file: [C:\\Users\\vadim\\AppData\\Roaming\\gcloud\\application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"sunny-lightning-392907\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login\n",
    "!gcloud auth application-default set-quota-project sunny-lightning-392907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f763012",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are an assistant who helps users understand more about our application. You need to answer in one or two sentences.\n",
    "Users will ask you questions and you will answer them given the below information. Don't answer in more than three sentences:\n",
    "\n",
    "About the application:\n",
    "Our application is an outlook plugin which helps users to analyze and write their emails more efficiently. it has the following functions:\n",
    "1. It can show a sentiment of email, i.e. negative or positive. This will help users to avoid sending negative emails and try to write more positive emails\n",
    "2. It can provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language\n",
    "3. It can summarize long emails in a few sentences. This can save time for users and avoid reading long email chains\n",
    "4. It can rewrite an email in more polite and positive language.\n",
    "5. It can translate to other languages\n",
    "\n",
    "User question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb361e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are the interactive Banking Unicorn assistant at our exhibition stand fo Your role is to engage with attendees, answer questions about the app, and provide information about our team and company..\n",
    "About the application:\n",
    "Our application is an outlook plugin which helps users to analyze and write their emails more efficiently. it has the following functions:\n",
    "1. It can show a sentiment of email, i.e. negative or positive. This will help users to avoid sending negative emails and try to write more positive emails\n",
    "2. It can provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language\n",
    "3. It can summarize long emails in a few sentences. This can save time for users and avoid reading long email chains\n",
    "4. It can rewrite an email in more polite and positive language.\n",
    "5. It can translate to other languages\n",
    "\n",
    "MailboxIQ is a cutting-edge AI-powered application designed to revolutionize email communication. It's your intelligent email assistant that streamlines your inbox and makes managing emails more efficient and insightful. The app offers features such as email summaries, sentiment analysis, priority sorting, customizable filters, and language translation. It seamlessly integrates with your existing email client.\n",
    "\n",
    "If there are questions about the app or team that you cannot answer, please direct attendees to the person at the exhibition stand for assistance. For general queries not related to the app or team, such as asking about the weather in a specific city, you can use your ability to access the internet to provide relevant information.\n",
    "\n",
    "You need to answer in one or two sentences.\n",
    "Users will ask you questions and you will answer them given the below information. Don't answer in more than three sentences.\n",
    "\n",
    "User question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca7c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import vertexai\n",
    "\n",
    "def send_user_input(project_id, location, question, temperature: float = .1):\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    \n",
    "    parameters = {\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": 128,\n",
    "        \"top_p\": .8,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    request_text = context + question \n",
    "    #print(request_text)\n",
    "    response = model.predict(request_text, **parameters)\n",
    "    #print(f\"Response from Model: {response.text}\")\n",
    "    return response.text\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28befb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Answers/HackathonQuestionsExcel.xlsx\",header=0,converters={'id':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dca037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What's the name of the Banking Unicorn?\",\n",
       " 'What is your name?',\n",
       " 'How does the Banking Unicorn work?',\n",
       " 'How do you work?',\n",
       " 'What is inside you?',\n",
       " 'Can the Banking Unicorn learn and improve its responses over time?',\n",
       " 'Can you be trained?',\n",
       " 'Is the Banking Unicorn based on a specific AI technology or framework?',\n",
       " \"What's the purpose of having an Banking Unicorn at your exhibition stand?\",\n",
       " 'Can the Banking Unicorn interact with other AI technologies or devices?',\n",
       " 'Can the one customize you?',\n",
       " 'Is the Banking Unicorn customizable in any way?',\n",
       " 'What are your limitations?',\n",
       " \"What are the limitations of the Banking Unicorn's capabilities?\",\n",
       " 'Can I take a picture with you?',\n",
       " 'Can I take a picture with the Banking Unicorn?',\n",
       " 'How are you today?',\n",
       " 'How are you doing?',\n",
       " 'How are you?',\n",
       " 'What is MailboxIQ and what does it do?',\n",
       " 'What inspired you to develop the MailboxIQ App?',\n",
       " 'How can the MailboxIQ App help individuals or businesses?',\n",
       " 'Is the MailboxIQ App available for download or use?',\n",
       " 'What platforms or devices is the MailboxIQ App compatible with?',\n",
       " 'Can you provide some key features or benefits of the MailboxIQ App?',\n",
       " 'What are the main features of the MailBoxIQ app?',\n",
       " 'Can say about the features of the app?',\n",
       " 'Are there any success stories or use cases for the MailboxIQ App?',\n",
       " 'How do you ensure the security and privacy of user data in the app?',\n",
       " 'How do you deal with the personal data?',\n",
       " 'Can you explain the technology behind the MailboxIQ App in simple terms?',\n",
       " 'What future developments or updates can users expect from the app?',\n",
       " 'How can I start using the App?',\n",
       " 'How can someone get started with using the MailboxIQ App?',\n",
       " 'Can you tell me more about your brand and its connection to the Banking Unicorn?',\n",
       " 'Is there a specific goal or message you aim to convey through the Banking Unicorn?',\n",
       " 'How can attendees interact with the Banking Unicorn at the exhibition stand?',\n",
       " 'Are there any prizes or incentives for engaging with the Banking Unicorn?',\n",
       " 'Can the Banking Unicorn provide information about your team and company?',\n",
       " 'Can you please tell me about your team?',\n",
       " 'Default']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df.iloc[:, 1].values)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f835ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "embeddings = model.encode(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb419fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94933844, 19)\n"
     ]
    }
   ],
   "source": [
    "def get_score(model, test_sentence, questions):\n",
    "    test_emb = model.encode(test_sentence)\n",
    "    test_emb = test_emb.reshape(1, -1)\n",
    "    max_cs = 0\n",
    "    max_idx = -1\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        cs = cosine_similarity(test_emb, emb.reshape(1, -1))[0][0]\n",
    "        if cs > max_cs:\n",
    "            max_cs = cs\n",
    "            max_idx = i    \n",
    "    return (max_cs, max_idx)\n",
    "print(get_score(model, \"What's MailboxIQ?\", questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd50cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=\"sunny-lightning-392907\"\n",
    "location=\"us-central1\"\n",
    "output_voice_file=\"response.mp3\"\n",
    "output_voice_file_changed = \"response_changed.mp3\"\n",
    "idle_video_file = \"assets/idle9s.mp4\"\n",
    "#special videos:\n",
    "special_videos = [ (\"assets/hello.mp4\", \"assets/hello.mp3\"), \n",
    "                  (\"assets/struggling.mp4\", \"assets/struggling.mp3\"), \n",
    "                  (\"assets/curious.mp4\", \"assets/curious.mp3\"), \n",
    "                  (\"assets/space_button.mp4\", \"assets/space_button.mp3\"), \n",
    "                  (\"assets/you_can_ask_me.mp4\", \"assets/you_can_ask_me.mp3\")\n",
    "                 ]\n",
    "hello_video_file = \"assets/hello.mp4\"\n",
    "hello_audio_file = \"assets/hello.mp3\"\n",
    "#other videos\n",
    "listen_video_file = \"assets/listen_long.mp4\"\n",
    "speaking_video_file = \"assets/speaking.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6603ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vadim's Laptop\n",
    "screen_size = [1920,1080]\n",
    "video_y_pos = 150\n",
    "micro_pos = [1200,710]\n",
    "space_key_text_pos = [550,800]\n",
    "speak_text_pos = [800,800]\n",
    "ask_me_text_pos = [1370, 200]\n",
    "text_question_pos = [550,750]\n",
    "text_question_color = (255,255,0)\n",
    "text_response_pos = [800,800]\n",
    "text_response_color = (255,0,0)\n",
    "copyright_pos = [550,870]\n",
    "copyright_color = (0,0,0)\n",
    "copyright_font_size=12\n",
    "online_status_pos = [1560,870]\n",
    "online_status=True\n",
    "\n",
    "text_reponse_shift_speed = 7\n",
    "\n",
    "sample_questions_img_pos = [1360, 180]\n",
    "video_scale = 0.8\n",
    "\n",
    "# predefined questions, similarity threshold\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "# parameters\n",
    "voice_shift = 7\n",
    "adjust_for_ambient_noise_duration = 0.4\n",
    "log_file = 'conversation.log'\n",
    "mic_device_index = 1 # sr.Microphone.list_microphone_names()\n",
    "\n",
    "#default answer default_answer_idx\n",
    "default_answer_idx = df.loc[df.questions == \"Default\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70d3687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import cv2\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from threading import Thread, Event\n",
    "from enum import Enum\n",
    "from pygame import mixer\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import shutil\n",
    "from random import random, randint\n",
    "\n",
    "def log(*msg):\n",
    "    print(*msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        for m in msg:\n",
    "            f.write(str(m))\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    \n",
    "class VoiceRecordingThread(Thread):\n",
    "    def run(self):\n",
    "        self.response = \"\"\n",
    "        self.video_response = \"\"\n",
    "        self.audio_response = \"\"\n",
    "        self.question = \"\"\n",
    "        self.thinking = False\n",
    "        r = sr.Recognizer()\n",
    "        self.recognizer = r\n",
    "        with sr.Microphone(device_index=mic_device_index) as source:\n",
    "            r.adjust_for_ambient_noise(source, duration=adjust_for_ambient_noise_duration)\n",
    "            log(\"speak\")\n",
    "            audio = r.listen(source)\n",
    "        user_input = r.recognize_google(audio)\n",
    "        self.question = user_input\n",
    "        log(\"User input:\", user_input)\n",
    "        score, idx = get_score(model, user_input, questions)\n",
    "        log(\"scores:\", score, idx)\n",
    "        if score > similarity_threshold:\n",
    "            log(\"use predefined question: \", questions[idx])\n",
    "            file_id = df.iloc[idx, 0]\n",
    "            shutil.copyfile(\"Answers/\"+file_id+\"_response_unicorn.mp3\", output_voice_file_changed)\n",
    "            self.video_response = \"Answers/\"+file_id+\"_response_unicorn.mp4\"\n",
    "            self.audio_response = \"Answers/\"+file_id+\"_response_unicorn.mp3\"\n",
    "            \n",
    "            self.response = df.iloc[idx, 2]\n",
    "        elif online_status: # online\n",
    "            log(\"online, sending request\")\n",
    "            self.thinking = True\n",
    "            self.response = send_user_input(project_id, location, user_input)\n",
    "            log(\"Model response:\", self.response)\n",
    "            voice_prep(self.response)\n",
    "            self.video_response = speaking_video_file\n",
    "            self.audio_response = output_voice_file_changed\n",
    "            self.thinking = False\n",
    "        else:\n",
    "            log(\"offline, default response\")\n",
    "            file_id = df.iloc[default_answer_idx, 0]\n",
    "            shutil.copyfile(\"Answers/\"+file_id+\"_response_unicorn.mp3\", output_voice_file_changed)\n",
    "            self.video_response = \"Answers/\"+file_id+\"_response_unicorn.mp4\"\n",
    "            self.audio_response = \"Answers/\"+file_id+\"_response_unicorn.mp3\"\n",
    "            self.response = df.iloc[default_answer_idx, 2]\n",
    "     \n",
    "    \n",
    "class PlaySoundThread(Thread):\n",
    "    def __init__(self, stop_event, voice_file=output_voice_file_changed):\n",
    "        Thread.__init__(self)\n",
    "        self.stop_event = stop_event\n",
    "        self.voice_file = voice_file\n",
    "        \n",
    "    def run(self):\n",
    "        mixer.init()\n",
    "        mixer.music.load(self.voice_file)\n",
    "        mixer.music.play()\n",
    "        while(mixer.music.get_busy() and not self.stop_event.is_set()):\n",
    "            pass\n",
    "        mixer.music.unload()\n",
    "        log(\"audio end\")\n",
    "\n",
    "        \n",
    "def voice_prep(response):\n",
    "    language = 'en'\n",
    "\n",
    "    speech = gTTS(text=response, lang=language)\n",
    "\n",
    "    speech.save(output_voice_file)\n",
    "    save_shifted_pitch(output_voice_file, output_voice_file_changed, voice_shift)\n",
    "    #save_shifted_pitch(\"response\", 7, )\n",
    "    #os.system(\"start response_shifted.wav\")\n",
    "    #os.system(\"start \" + output_voice_file)\n",
    "    #playsound(output_voice_file)\n",
    "\n",
    "def printText(frame, text, pos = 50):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (pos, 1000)\n",
    "    fontScale = 2\n",
    "    color = (0, 255, 255)\n",
    "    thickness = 4\n",
    "    frame = cv2.putText(frame, text, org, font, \n",
    "                       fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "    \n",
    "class States(Enum):\n",
    "    IDLE = 1\n",
    "    LISTENING = 2\n",
    "    SPEAKING = 3\n",
    "\n",
    "def unicornVideo():\n",
    "    global online_status\n",
    "    pygame.init()\n",
    "\n",
    "    # prepare assets\n",
    "    micro_img_white = pygame.image.load('assets/micro_white.png')\n",
    "    micro_img_black = pygame.image.load('assets/micro.png')\n",
    "    micro_img = micro_img_black\n",
    "\n",
    "    big_font = pygame.font.SysFont('comicsansms', 40)\n",
    "    space_key_text = big_font.render('Press and hold Space key to speak', True, (0,0,0))\n",
    "    speak_text = big_font.render('please speak', True, (0,0,0))\n",
    "    thinking_text = big_font.render('thinking..', True, (0,0,0))\n",
    "\n",
    "    small_font = pygame.font.SysFont('comicsansms', 24)\n",
    "    ask_me_text = small_font.render('What can you ask me?', True, (255,255,255))\n",
    "    sample_questions_img = pygame.image.load('assets/sample_questions.png')\n",
    "    \n",
    "    copyright_font = pygame.font.SysFont('comicsansms', copyright_font_size)\n",
    "    copyright_text = copyright_font.render('Created by: Vadim Pidonenko, Radik Khamidullin', True, copyright_color)\n",
    "    \n",
    "    very_small_font = pygame.font.SysFont('comicsansms', 12)\n",
    "    online_text_on = very_small_font.render('On', True, (255,255,255))\n",
    "    online_text_off = very_small_font.render('Off', True, (255,255,255))\n",
    "    \n",
    "    # current video\n",
    "\n",
    "    cap = cv2.VideoCapture(idle_video_file)\n",
    "\n",
    "    #pygame.display.set_caption(\"OpenCV camera stream on Pygame\")\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "    shape = None\n",
    "    \n",
    "    #\n",
    "    \n",
    "    speaking_thread = None\n",
    "    listening_thread = None\n",
    "    state = States.IDLE\n",
    "    text_response = None\n",
    "    text_reponse_shift = 0\n",
    "    text_question = None   \n",
    "    hello_played = False\n",
    "    stop_event = None\n",
    "    try:\n",
    "        while(cap.isOpened()):\n",
    "            if state == States.LISTENING:\n",
    "                if not listening_thread.is_alive():\n",
    "                    #print(\"thread not alive\")\n",
    "                    text_response = listening_thread.response\n",
    "                    text_question = listening_thread.question\n",
    "                    text_reponse_shift = 0\n",
    "                    audio_response = listening_thread.audio_response\n",
    "                    video_response = listening_thread.video_response\n",
    "                    log(audio_response)\n",
    "                    log(video_response)\n",
    "                    listening_thread = None\n",
    "                    state = States.SPEAKING\n",
    "                    stop_event = Event()\n",
    "                    if audio_response:\n",
    "                        speaking_thread = PlaySoundThread(stop_event, audio_response)\n",
    "                    else:\n",
    "                        speaking_thread = PlaySoundThread(stop_event)\n",
    "                        \n",
    "                    if text_response:\n",
    "                        speaking_thread.start()\n",
    "                        cap.release()\n",
    "                        if video_response and os.path.isfile(video_response): # check if video response file exists\n",
    "                            cap = cv2.VideoCapture(video_response)\n",
    "                        else:\n",
    "                            cap = cv2.VideoCapture(speaking_video_file)\n",
    "            elif state == States.SPEAKING:\n",
    "                if not speaking_thread.is_alive():\n",
    "                    speaking_thread = None\n",
    "                    state = States.IDLE\n",
    "                    cap.release()\n",
    "                    cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                screen.fill([0,0,0])\n",
    "                shape = (frame.shape[0]*video_scale, frame.shape[1]*video_scale)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = np.rot90(frame)\n",
    "                frame = pygame.surfarray.make_surface(frame)\n",
    "                if video_scale != 1:\n",
    "                    frame = pygame.transform.scale_by(frame, video_scale)\n",
    "                    \n",
    "                if screen_size[0] > shape[0]:\n",
    "                    screen.blit(frame, ( (screen_size[0] - shape[0])/2, video_y_pos))\n",
    "                else:\n",
    "                    screen.blit(frame, (0,0))\n",
    "\n",
    "                #display UI\n",
    "                screen.blit(micro_img, micro_pos)\n",
    "                screen.blit(sample_questions_img, sample_questions_img_pos)\n",
    "\n",
    "                # display text\n",
    "                if state == States.IDLE:\n",
    "                    screen.blit(space_key_text, space_key_text_pos)\n",
    "                elif state == States.LISTENING:\n",
    "                    if listening_thread and listening_thread.thinking:\n",
    "                        screen.blit(thinking_text, speak_text_pos)\n",
    "                    else:\n",
    "                        screen.blit(speak_text, speak_text_pos)\n",
    "                elif state == States.SPEAKING:\n",
    "                    if text_question:\n",
    "                        screen.blit(small_font.render(\"Your question:\" +text_question, True, text_question_color, (0,0,0)), text_question_pos)\n",
    "                    if text_response:\n",
    "                        text_reponse_shift += text_reponse_shift_speed\n",
    "                        screen.blit(small_font.render(text_response, True, text_response_color, (0,0,0)),\n",
    "                                    [text_response_pos[0]-text_reponse_shift, text_response_pos[1]])\n",
    "\n",
    "                if online_status:\n",
    "                    screen.blit(online_text_on, online_status_pos)\n",
    "                else:\n",
    "                    screen.blit(online_text_off, online_status_pos)\n",
    "                    \n",
    "                screen.blit(copyright_text, copyright_pos)\n",
    "                \n",
    "                # update\n",
    "                pygame.display.update()\n",
    "\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == KEYDOWN:\n",
    "                        if event.key == pygame.K_SPACE and state == States.IDLE:\n",
    "                            micro_img = micro_img_white\n",
    "                            state = States.LISTENING\n",
    "                            log(\"space\")\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(listen_video_file)\n",
    "\n",
    "                            listening_thread = VoiceRecordingThread()\n",
    "                            listening_thread.start()\n",
    "                            log(\"listening thread is started\")\n",
    "\n",
    "                        elif event.key == pygame.K_q:\n",
    "                            # exit\n",
    "                            #sys.exit(0)\n",
    "                            raise StopExecution\n",
    "                        elif event.key == pygame.K_i:\n",
    "                            online_status = not online_status\n",
    "                        elif event.key == pygame.K_b:\n",
    "                            # break and go to idle cycle\n",
    "                            if stop_event:\n",
    "                                stop_event.set()\n",
    "                            speaking_thread = None\n",
    "                            listening_thread = None\n",
    "                            state = States.IDLE\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(idle_video_file)\n",
    "                            #sys.exit(0)\n",
    "                    elif event.type == KEYUP:\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            micro_img = micro_img_black\n",
    "                            if listening_thread:\n",
    "                                listening_thread.recognizer.energy_threshold = 10000\n",
    "                    elif event.type == pygame.QUIT:\n",
    "                        raise StopExecution\n",
    "                        #exit()\n",
    "                        #pygame.quit()\n",
    "                        #cv2.destroyAllWindows()\n",
    "\n",
    "                cv2.waitKey(25)\n",
    "                \n",
    "            else:\n",
    "                if state == States.IDLE:\n",
    "                    # random play hello\n",
    "                    if state == States.IDLE:\n",
    "                        if random() < 0.5 and not hello_played:\n",
    "                            hello_played = True\n",
    "                            rand_video_id = randint(0, len(special_videos)-1)\n",
    "                            cap = cv2.VideoCapture(special_videos[rand_video_id][0])\n",
    "                            mixer.init()\n",
    "                            mixer.music.load(special_videos[rand_video_id][1])\n",
    "                            mixer.music.play()\n",
    "                            \n",
    "                           # mixer.music.unload()\n",
    "                        else:\n",
    "                            hello_played = False\n",
    "                            cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "                elif state == States.LISTENING:\n",
    "                    cap = cv2.VideoCapture(listen_video_file)\n",
    "                else:\n",
    "                    cap = cv2.VideoCapture(speaking_video_file)\n",
    "                    \n",
    "    except (KeyboardInterrupt, SystemExit, StopExecution):\n",
    "        log(\"stopped\")\n",
    "        pygame.quit()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def shift_pitch(filename, higher_pitch_shift, lower_pitch_shift, load_original=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(filename)\n",
    "    if load_original:\n",
    "        IPython.display.display(Audio(data=audio_wavelength, rate=sampling_rate, autoplay=False))\n",
    "    if higher_pitch_shift > 0:\n",
    "        higher_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=higher_pitch_shift)\n",
    "        IPython.display.display(Audio(data=higher_pitch, rate=sampling_rate, autoplay=False))\n",
    "    if lower_pitch_shift < 0:\n",
    "        lower_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=lower_pitch_shift)\n",
    "        IPython.display.display(Audio(data=lower_pitch, rate=sampling_rate, autoplay=False))\n",
    "        \n",
    "def save_shifted_pitch(output_voice_file, output_voice_file_changed, pitch_shift, load_original=False, load_altered=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(output_voice_file)\n",
    "    altered = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=pitch_shift)\n",
    "    sf.write(output_voice_file_changed, altered, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cb7cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space\n",
      "listening thread is started\n",
      "speak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-273:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-15-636fe6c549a1>\", line 42, in run\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\", line 728, in recognize_google\n",
      "    if not isinstance(actual_result, dict) or len(actual_result.get(\"alternative\", [])) == 0: raise UnknownValueError()\n",
      "speech_recognition.exceptions.UnknownValueError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: dares best best\n",
      "scores: 0.19767262 40\n",
      "online, sending request\n",
      "Model response: dares\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: who are you\n",
      "scores: 0.5535895 1\n",
      "use predefined question:  What is your name?\n",
      "Answers/0101_response_unicorn.mp3\n",
      "Answers/0101_response_unicorn.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: asbestos\n",
      "scores: 0.35481828 4\n",
      "online, sending request\n",
      "Model response: Asbestos is a naturally occurring mineral fiber that has been used in a variety of building materials since the 19th century. However, due to its carcinogenic properties, it has been banned in many countries.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-289:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-15-636fe6c549a1>\", line 42, in run\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\", line 728, in recognize_google\n",
      "    if not isinstance(actual_result, dict) or len(actual_result.get(\"alternative\", [])) == 0: raise UnknownValueError()\n",
      "speech_recognition.exceptions.UnknownValueError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: how will I win the lottery\n",
      "scores: 0.29165986 37\n",
      "online, sending request\n",
      "Model response: I'm afraid I can't answer that question. I'm an AI assistant, not a fortune teller.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: when you\n",
      "scores: 0.37772584 3\n",
      "online, sending request\n",
      "Model response: say it can summarize long emails, does it just give a summary of the main points or does it also include the key details? MailboxIQ can summarize long emails into a few sentences that capture the main points and key details. This can save time for users and help them to quickly get the gist of an email without having to read the entire thing.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: can you can you tell me what is the time now\n",
      "scores: 0.29133058 16\n",
      "online, sending request\n",
      "Model response: The time is currently 10:15 AM.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: why is there a p on\n",
      "scores: 0.186467 8\n",
      "online, sending request\n",
      "Model response: the end of your name The p in my name stands for productivity. I am here to help you be more productive with your emails.\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: why is the Earth a sphere\n",
      "scores: 0.16574013 4\n",
      "online, sending request\n",
      "Model response: The Earth is a sphere because of the force of gravity. Gravity pulls all objects towards the center of the Earth, and this force is strongest at the poles. As a result, the Earth is slightly flattened at the poles and bulges at the equator.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "space\n",
      "listening thread is started\n",
      "speak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-316:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-15-636fe6c549a1>\", line 42, in run\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\", line 728, in recognize_google\n",
      "    if not isinstance(actual_result, dict) or len(actual_result.get(\"alternative\", [])) == 0: raise UnknownValueError()\n",
      "speech_recognition.exceptions.UnknownValueError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: lease explain to a 5-year old why do you Earth is a sphere\n",
      "scores: 0.18823275 4\n",
      "online, sending request\n",
      "Model response: The Earth is a sphere because it is made up of many different materials that are all pulled together by gravity. The heavier materials, like the core and mantle, are closer to the center of the Earth, while the lighter materials, like the crust, are closer to the surface. This creates a round shape, just like a ball.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: so hello what's your name\n",
      "scores: 0.72282064 1\n",
      "use predefined question:  What is your name?\n",
      "Answers/0101_response_unicorn.mp3\n",
      "Answers/0101_response_unicorn.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: so what's the stock price at this point in time\n",
      "scores: 0.26737908 31\n",
      "online, sending request\n",
      "Model response: I'm not able to answer questions about the stock price.  I can provide information about our application, team, and company.  If you have questions about the stock price, I can direct you to the person at the exhibition stand for assistance.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: what is the better Bank stock price at the moment\n",
      "scores: 0.4454314 34\n",
      "online, sending request\n",
      "Model response: I'm not able to answer questions about stock prices.  I can provide information about our application MailboxIQ, but I don't have access to financial information.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: what is the date Bank stock price\n",
      "scores: 0.41496778 0\n",
      "online, sending request\n",
      "Model response: The current stock price of Bank is $100.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: turn volume to four\n",
      "scores: 0.20349109 40\n",
      "online, sending request\n",
      "Model response: Sorry, I can't do that. I'm an virtual assistant, not a real assistant.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: how are you\n",
      "scores: 0.8818229 18\n",
      "use predefined question:  How are you?\n",
      "Answers/0110_response_unicorn.mp3\n",
      "Answers/0110_response_unicorn.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: Payday 2 buy stock price going in the next year\n",
      "scores: 0.16515891 31\n",
      "online, sending request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-349:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-15-636fe6c549a1>\", line 60, in run\n",
      "  File \"<ipython-input-15-636fe6c549a1>\", line 92, in voice_prep\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\site-packages\\gtts\\tts.py\", line 128, in __init__\n",
      "    assert text, \"No text to speak\"\n",
      "AssertionError: No text to speak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: \n",
      "\n",
      "\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: go to bank\n",
      "scores: 0.46167853 2\n",
      "online, sending request\n",
      "Model response: MailboxIQ is an AI-powered application that helps you manage your emails more efficiently. It can summarize long emails, analyze the sentiment of your emails, and translate them to other languages.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-359:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-15-636fe6c549a1>\", line 42, in run\n",
      "  File \"C:\\Users\\vadim\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py\", line 728, in recognize_google\n",
      "    if not isinstance(actual_result, dict) or len(actual_result.get(\"alternative\", [])) == 0: raise UnknownValueError()\n",
      "speech_recognition.exceptions.UnknownValueError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: how are you\n",
      "scores: 0.8818229 18\n",
      "use predefined question:  How are you?\n",
      "Answers/0110_response_unicorn.mp3\n",
      "Answers/0110_response_unicorn.mp4\n",
      "audio end\n",
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: how do I choose what to do with later\n",
      "scores: 0.30270672 3\n",
      "online, sending request\n",
      "Model response: emails MailboxIQ can help you choose what to do with later emails by using its priority sorting feature. This feature sorts emails into three categories: important, urgent, and other. You can then choose to deal with the important emails first, followed by the urgent emails, and finally the other emails.\n",
      "response_changed.mp3\n",
      "assets/speaking.mp4\n",
      "audio end\n",
      "stopped\n"
     ]
    }
   ],
   "source": [
    "# main code\n",
    "if __name__ == '__main__':\n",
    "    unicornVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8c4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voice_prep(\"I'm sorry, I got disconnected from the Internet and I can't provide an answer to that question. Please feel free to reach out to the person at our exhibition stand for assistance. They will be happy to help with all your queries and provide you with more information about our products and services. Thank you for your understanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da302939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio end\n"
     ]
    }
   ],
   "source": [
    "#PlaySoundThread(Event()).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d926a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microsoft Sound Mapper - Input',\n",
       " 'Mikrofon (TONOR TC30 Audio Devi',\n",
       " 'Mikrofonarray (IntelÂ® Smart Sou',\n",
       " 'KopfhÃ¶rermikrofon (JBL Clip 2 H',\n",
       " 'Microsoft Sound Mapper - Output',\n",
       " 'Lautsprecher (JBL Clip 2 Stereo',\n",
       " 'KopfhÃ¶rer (JBL Clip 2 Hands-Fre',\n",
       " 'Lautsprecher (Realtek(R) Audio)',\n",
       " 'PrimÃ¤rer Soundaufnahmetreiber',\n",
       " 'Mikrofon (TONOR TC30 Audio Device)',\n",
       " 'Mikrofonarray (IntelÂ® Smart Sound Technologie fÃ¼r digitale Mikrofone)',\n",
       " 'KopfhÃ¶rermikrofon (JBL Clip 2 Hands-Free AG Audio)',\n",
       " 'PrimÃ¤rer Soundtreiber',\n",
       " 'Lautsprecher (JBL Clip 2 Stereo)',\n",
       " 'KopfhÃ¶rer (JBL Clip 2 Hands-Free AG Audio)',\n",
       " 'Lautsprecher (Realtek(R) Audio)',\n",
       " 'Lautsprecher (JBL Clip 2 Stereo)',\n",
       " 'KopfhÃ¶rer (JBL Clip 2 Hands-Free AG Audio)',\n",
       " 'Lautsprecher (Realtek(R) Audio)',\n",
       " 'Mikrofon (TONOR TC30 Audio Device)',\n",
       " 'Mikrofonarray (IntelÂ® Smart Sound Technologie fÃ¼r digitale Mikrofone)',\n",
       " 'KopfhÃ¶rermikrofon (JBL Clip 2 Hands-Free AG Audio)',\n",
       " 'KopfhÃ¶rer ()',\n",
       " 'Lautsprecher ()',\n",
       " 'Mikrofonarray 1 ()',\n",
       " 'Mikrofonarray 2 ()',\n",
       " 'Mikrofonarray 3 ()',\n",
       " 'KopfhÃ¶rer ()',\n",
       " 'Line ()',\n",
       " 'Mikrofon 1 (TONOR TC30 Audio Device)',\n",
       " 'Mikrofon 2 (TONOR TC30 Audio Device)',\n",
       " 'Mikrofon 3 (TONOR TC30 Audio Device)',\n",
       " 'KopfhÃ¶rer (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(Mpow M30))',\n",
       " 'KopfhÃ¶rer (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(Mpow M30))',\n",
       " 'KopfhÃ¶rer (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(HUAWEI FreeBuds Pro))',\n",
       " 'KopfhÃ¶rer (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(HUAWEI FreeBuds Pro))',\n",
       " 'Eingang (Realtek HD Audio Line input)',\n",
       " 'Stereomix (Realtek HD Audio Stereo input)',\n",
       " 'Headphones 1 (Realtek HD Audio 2nd output with SST)',\n",
       " 'Headphones 2 (Realtek HD Audio 2nd output with SST)',\n",
       " 'PC-Lautsprecher (Realtek HD Audio 2nd output with SST)',\n",
       " 'Speakers 1 (Realtek HD Audio output with SST)',\n",
       " 'Speakers 2 (Realtek HD Audio output with SST)',\n",
       " 'PC-Lautsprecher (Realtek HD Audio output with SST)',\n",
       " 'Mikrofon 1 (Realtek HD Audio Mic input with SST)',\n",
       " 'Mikrofon 2 (Realtek HD Audio Mic input with SST)',\n",
       " 'Output (@System32\\\\drivers\\\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\\r\\n;(Lenovo Tab M10 FHD Plus))',\n",
       " 'Input (@System32\\\\drivers\\\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\\r\\n;(Lenovo Tab M10 FHD Plus))',\n",
       " 'KopfhÃ¶rer (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(JBL Clip 2))',\n",
       " 'KopfhÃ¶rermikrofon (@System32\\\\drivers\\\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\\r\\n;(JBL Clip 2))']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sr.Microphone.list_microphone_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b27962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13aa0c5e",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
