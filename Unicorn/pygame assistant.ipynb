{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "115502d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default login\n",
    "!gcloud auth application-default set-quota-project sunny-lightning-392907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f763012",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are an assistant who helps users understand more about our application. You need to answer in one or two sentences.\n",
    "Users will ask you questions and you will answer them given the below information. Don't answer in more than three sentences:\n",
    "\n",
    "About the application:\n",
    "Our application is an outlook plugin which helps users to analyze and write their emails more efficiently. it has the following functions:\n",
    "1. It can show a sentiment of email, i.e. negative or positive. This will help users to avoid sending negative emails and try to write more positive emails\n",
    "2. It can provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language\n",
    "3. It can summarize long emails in a few sentences. This can save time for users and avoid reading long email chains\n",
    "4. It can rewrite an email in more polite and positive language.\n",
    "5. It can translate to other languages\n",
    "\n",
    "User question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb361e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are the interactive Banking Unicorn assistant at our exhibition stand fo Your role is to engage with attendees, answer questions about the app, and provide information about our team and company..\n",
    "About the application:\n",
    "Our application is an outlook plugin which helps users to analyze and write their emails more efficiently. it has the following functions:\n",
    "1. It can show a sentiment of email, i.e. negative or positive. This will help users to avoid sending negative emails and try to write more positive emails\n",
    "2. It can provide a language tone of email, this should help users to avoid impolite words and avoid using inappropriate language\n",
    "3. It can summarize long emails in a few sentences. This can save time for users and avoid reading long email chains\n",
    "4. It can rewrite an email in more polite and positive language.\n",
    "5. It can translate to other languages\n",
    "\n",
    "MailboxIQ is a cutting-edge AI-powered application designed to revolutionize email communication. It's your intelligent email assistant that streamlines your inbox and makes managing emails more efficient and insightful. The app offers features such as email summaries, sentiment analysis, priority sorting, customizable filters, and language translation. It seamlessly integrates with your existing email client.\n",
    "\n",
    "If there are questions about the app or team that you cannot answer, please direct attendees to the person at the exhibition stand for assistance. For general queries not related to the app or team, such as asking about the weather in a specific city, you can use your ability to access the internet to provide relevant information.\n",
    "\n",
    "You need to answer in one or two sentences.\n",
    "Users will ask you questions and you will answer them given the below information. Don't answer in more than three sentences.\n",
    "\n",
    "User question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca7c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import vertexai\n",
    "\n",
    "def send_user_input(project_id, location, question, temperature: float = .1):\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    \n",
    "    parameters = {\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": 128,\n",
    "        \"top_p\": .8,\n",
    "        \"top_k\": 40,\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    request_text = context + question \n",
    "    #print(request_text)\n",
    "    response = model.predict(request_text, **parameters)\n",
    "    #print(f\"Response from Model: {response.text}\")\n",
    "    return response.text\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28befb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Answers/HackathonQuestionsExcel.xlsx\",header=0,converters={'id':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11dca037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What's the name of the Banking Unicorn?\",\n",
       " \"What's your name?\",\n",
       " 'What is your name?',\n",
       " 'How does the Banking Unicorn work?',\n",
       " 'How do you work?',\n",
       " 'What is inside you?',\n",
       " 'Can the Banking Unicorn learn and improve its responses over time?',\n",
       " 'Is the Banking Unicorn based on a specific AI technology or framework?',\n",
       " \"What's the purpose of having an Banking Unicorn at your exhibition stand?\",\n",
       " 'Can the Banking Unicorn interact with other AI technologies or devices?',\n",
       " 'Can the one customize you?',\n",
       " 'Is the Banking Unicorn customizable in any way?',\n",
       " 'What are your limitations?',\n",
       " \"What are the limitations of the Banking Unicorn's capabilities?\",\n",
       " 'Can I take a picture with you?',\n",
       " 'Can I take a picture with the Banking Unicorn?',\n",
       " 'How are you today?',\n",
       " 'How are you doing?',\n",
       " 'How are you?',\n",
       " 'What is MailboxIQ and what does it do?',\n",
       " 'What inspired you to develop the MailboxIQ App?',\n",
       " 'How can the MailboxIQ App help individuals or businesses?',\n",
       " 'Is the MailboxIQ App available for download or use?',\n",
       " 'What platforms or devices is the MailboxIQ App compatible with?',\n",
       " 'Can you provide some key features or benefits of the MailboxIQ App?',\n",
       " 'Are there any success stories or use cases for the MailboxIQ App?',\n",
       " 'How do you ensure the security and privacy of user data in the app?',\n",
       " 'How do you deal with the personal data?',\n",
       " 'Can you explain the technology behind the MailboxIQ App in simple terms?',\n",
       " 'What future developments or updates can users expect from the app?',\n",
       " 'How can someone get started with using the MailboxIQ App?',\n",
       " 'Can you tell me more about your brand and its connection to the Banking Unicorn?',\n",
       " 'Is there a specific goal or message you aim to convey through the Banking Unicorn?',\n",
       " 'How can attendees interact with the Banking Unicorn at the exhibition stand?',\n",
       " 'Are there any prizes or incentives for engaging with the Banking Unicorn?',\n",
       " 'Can the Banking Unicorn provide information about your team and company?',\n",
       " 'Can you please tell me about your team?',\n",
       " 'Default']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df.iloc[:, 1].values)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f835ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('stsb-roberta-large')\n",
    "\n",
    "embeddings = model.encode(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb419fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94933844, 19)\n"
     ]
    }
   ],
   "source": [
    "def get_score(model, test_sentence, questions):\n",
    "    test_emb = model.encode(test_sentence)\n",
    "    test_emb = test_emb.reshape(1, -1)\n",
    "    max_cs = 0\n",
    "    max_idx = -1\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        cs = cosine_similarity(test_emb, emb.reshape(1, -1))[0][0]\n",
    "        if cs > max_cs:\n",
    "            max_cs = cs\n",
    "            max_idx = i    \n",
    "    return (max_cs, max_idx)\n",
    "print(get_score(model, \"What's MailboxIQ?\", questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd50cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=\"sunny-lightning-392907\"\n",
    "location=\"us-central1\"\n",
    "output_voice_file=\"response.mp3\"\n",
    "output_voice_file_changed = \"response_changed.mp3\"\n",
    "idle_video_file = \"assets/idle9s.mp4\"\n",
    "#special videos:\n",
    "special_videos = [ (\"assets/hello.mp4\", \"assets/hello.mp3\"), \n",
    "                  (\"assets/struggling.mp4\", \"assets/struggling.mp3\"), \n",
    "                  (\"assets/curious.mp4\", \"assets/curious.mp3\"), \n",
    "                  (\"assets/space_button.mp4\", \"assets/space_button.mp3\"), \n",
    "                  (\"assets/you_can_ask_me.mp4\", \"assets/you_can_ask_me.mp3\")\n",
    "                 ]\n",
    "hello_video_file = \"assets/hello.mp4\"\n",
    "hello_audio_file = \"assets/hello.mp3\"\n",
    "#other videos\n",
    "listen_video_file = \"assets/listen_long.mp4\"\n",
    "speaking_video_file = \"assets/speaking.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6603ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vadim's Laptop\n",
    "screen_size = [1920,1080]\n",
    "video_y_pos = 150\n",
    "micro_pos = [1200,710]\n",
    "space_key_text_pos = [550,800]\n",
    "speak_text_pos = [800,800]\n",
    "ask_me_text_pos = [1370, 200]\n",
    "text_question_pos = [550,750]\n",
    "text_question_color = (255,255,0)\n",
    "text_response_pos = [800,800]\n",
    "text_response_color = (255,0,0)\n",
    "copyright_pos = [550,870]\n",
    "copyright_color = (0,0,0)\n",
    "copyright_font_size=12\n",
    "online_status_pos = [1560,870]\n",
    "online_status=True\n",
    "\n",
    "text_reponse_shift_speed = 7\n",
    "\n",
    "sample_questions_img_pos = [1360, 180]\n",
    "video_scale = 0.8\n",
    "\n",
    "# predefined questions, similarity threshold\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "# parameters\n",
    "voice_shift = 7\n",
    "adjust_for_ambient_noise_duration = 0.2\n",
    "log_file = 'conversation.log'\n",
    "mic_device_index = 1 # sr.Microphone.list_microphone_names()\n",
    "\n",
    "#default answer default_answer_idx\n",
    "default_answer_idx = df.loc[df.questions == \"Default\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70d3687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from pygame.locals import *\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "import cv2\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from threading import Thread, Event\n",
    "from enum import Enum\n",
    "from pygame import mixer\n",
    "import os\n",
    "import speech_recognition as sr\n",
    "import shutil\n",
    "from random import random, randint\n",
    "\n",
    "def log(*msg):\n",
    "    print(*msg)\n",
    "    with open(log_file, 'a') as f:\n",
    "        for m in msg:\n",
    "            f.write(str(m))\n",
    "            f.write(' ')\n",
    "        f.write('\\n')\n",
    "    \n",
    "class VoiceRecordingThread(Thread):\n",
    "    def run(self):\n",
    "        self.response = \"\"\n",
    "        self.video_response = \"\"\n",
    "        self.audio_response = \"\"\n",
    "        self.question = \"\"\n",
    "        self.thinking = False\n",
    "        r = sr.Recognizer()\n",
    "        self.recognizer = r\n",
    "        with sr.Microphone(device_index=mic_device_index) as source:\n",
    "            r.adjust_for_ambient_noise(source, duration=adjust_for_ambient_noise_duration)\n",
    "            log(\"speak\")\n",
    "            audio = r.listen(source)\n",
    "        user_input = r.recognize_google(audio)\n",
    "        self.question = user_input\n",
    "        log(\"User input:\", user_input)\n",
    "        score, idx = get_score(model, user_input, questions)\n",
    "        log(\"scores:\", score, idx)\n",
    "        if score > similarity_threshold:\n",
    "            log(\"use predefined question: \", questions[idx])\n",
    "            file_id = df.iloc[idx, 0]\n",
    "            shutil.copyfile(\"Answers/\"+file_id+\"_response_unicorn.mp3\", output_voice_file_changed)\n",
    "            self.video_response = \"Answers/\"+file_id+\"_response_unicorn.mp4\"\n",
    "            self.audio_response = \"Answers/\"+file_id+\"_response_unicorn.mp3\"\n",
    "            \n",
    "            self.response = df.iloc[idx, 2]\n",
    "        elif online_status: # online\n",
    "            log(\"online, sending request\")\n",
    "            self.thinking = True\n",
    "            self.response = send_user_input(project_id, location, user_input)\n",
    "            log(\"Model response:\", self.response)\n",
    "            voice_prep(self.response)\n",
    "            self.video_response = speaking_video_file\n",
    "            self.audio_response = output_voice_file_changed\n",
    "            self.thinking = False\n",
    "        else:\n",
    "            log(\"offline, default response\")\n",
    "            file_id = df.iloc[default_answer_idx, 0]\n",
    "            shutil.copyfile(\"Answers/\"+file_id+\"_response_unicorn.mp3\", output_voice_file_changed)\n",
    "            self.video_response = \"Answers/\"+file_id+\"_response_unicorn.mp4\"\n",
    "            self.audio_response = \"Answers/\"+file_id+\"_response_unicorn.mp3\"\n",
    "            self.response = df.iloc[default_answer_idx, 2]\n",
    "     \n",
    "    \n",
    "class PlaySoundThread(Thread):\n",
    "    def __init__(self, stop_event, voice_file=output_voice_file_changed):\n",
    "        Thread.__init__(self)\n",
    "        self.stop_event = stop_event\n",
    "        self.voice_file = voice_file\n",
    "        \n",
    "    def run(self):\n",
    "        mixer.init()\n",
    "        mixer.music.load(self.voice_file)\n",
    "        mixer.music.play()\n",
    "        while(mixer.music.get_busy() and not self.stop_event.is_set()):\n",
    "            pass\n",
    "        mixer.music.unload()\n",
    "        log(\"audio end\")\n",
    "\n",
    "        \n",
    "def voice_prep(response):\n",
    "    language = 'en'\n",
    "\n",
    "    speech = gTTS(text=response, lang=language)\n",
    "\n",
    "    speech.save(output_voice_file)\n",
    "    save_shifted_pitch(output_voice_file, output_voice_file_changed, voice_shift)\n",
    "    #save_shifted_pitch(\"response\", 7, )\n",
    "    #os.system(\"start response_shifted.wav\")\n",
    "    #os.system(\"start \" + output_voice_file)\n",
    "    #playsound(output_voice_file)\n",
    "\n",
    "def printText(frame, text, pos = 50):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (pos, 1000)\n",
    "    fontScale = 2\n",
    "    color = (0, 255, 255)\n",
    "    thickness = 4\n",
    "    frame = cv2.putText(frame, text, org, font, \n",
    "                       fontScale, color, thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "    \n",
    "class States(Enum):\n",
    "    IDLE = 1\n",
    "    LISTENING = 2\n",
    "    SPEAKING = 3\n",
    "\n",
    "def unicornVideo():\n",
    "    global online_status\n",
    "    pygame.init()\n",
    "\n",
    "    # prepare assets\n",
    "    micro_img_white = pygame.image.load('assets/micro_white.png')\n",
    "    micro_img_black = pygame.image.load('assets/micro.png')\n",
    "    micro_img = micro_img_black\n",
    "\n",
    "    big_font = pygame.font.SysFont('comicsansms', 40)\n",
    "    space_key_text = big_font.render('Press and hold Space key to speak', True, (0,0,0))\n",
    "    speak_text = big_font.render('please speak', True, (0,0,0))\n",
    "    thinking_text = big_font.render('thinking..', True, (0,0,0))\n",
    "\n",
    "    small_font = pygame.font.SysFont('comicsansms', 24)\n",
    "    ask_me_text = small_font.render('What can you ask me?', True, (255,255,255))\n",
    "    sample_questions_img = pygame.image.load('assets/sample_questions.png')\n",
    "    \n",
    "    copyright_font = pygame.font.SysFont('comicsansms', copyright_font_size)\n",
    "    copyright_text = copyright_font.render('Created by: Vadim Pidonenko, Radik Khamidullin', True, copyright_color)\n",
    "    \n",
    "    very_small_font = pygame.font.SysFont('comicsansms', 12)\n",
    "    online_text_on = very_small_font.render('On', True, (255,255,255))\n",
    "    online_text_off = very_small_font.render('Off', True, (255,255,255))\n",
    "    \n",
    "    # current video\n",
    "\n",
    "    cap = cv2.VideoCapture(idle_video_file)\n",
    "\n",
    "    #pygame.display.set_caption(\"OpenCV camera stream on Pygame\")\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "    shape = None\n",
    "    \n",
    "    #\n",
    "    \n",
    "    speaking_thread = None\n",
    "    listening_thread = None\n",
    "    state = States.IDLE\n",
    "    text_response = None\n",
    "    text_reponse_shift = 0\n",
    "    text_question = None   \n",
    "    hello_played = False\n",
    "    stop_event = None\n",
    "    try:\n",
    "        while(cap.isOpened()):\n",
    "            if state == States.LISTENING:\n",
    "                if not listening_thread.is_alive():\n",
    "                    #print(\"thread not alive\")\n",
    "                    text_response = listening_thread.response\n",
    "                    text_question = listening_thread.question\n",
    "                    text_reponse_shift = 0\n",
    "                    audio_response = listening_thread.audio_response\n",
    "                    video_response = listening_thread.video_response\n",
    "                    log(audio_response)\n",
    "                    log(video_response)\n",
    "                    listening_thread = None\n",
    "                    state = States.SPEAKING\n",
    "                    stop_event = Event()\n",
    "                    if audio_response:\n",
    "                        speaking_thread = PlaySoundThread(stop_event, audio_response)\n",
    "                    else:\n",
    "                        speaking_thread = PlaySoundThread(stop_event)\n",
    "                        \n",
    "                    if text_response:\n",
    "                        speaking_thread.start()\n",
    "                        cap.release()\n",
    "                        if video_response and os.path.isfile(video_response): # check if video response file exists\n",
    "                            cap = cv2.VideoCapture(video_response)\n",
    "                        else:\n",
    "                            cap = cv2.VideoCapture(speaking_video_file)\n",
    "            elif state == States.SPEAKING:\n",
    "                if not speaking_thread.is_alive():\n",
    "                    speaking_thread = None\n",
    "                    state = States.IDLE\n",
    "                    cap.release()\n",
    "                    cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                screen.fill([0,0,0])\n",
    "                shape = (frame.shape[0]*video_scale, frame.shape[1]*video_scale)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = np.rot90(frame)\n",
    "                frame = pygame.surfarray.make_surface(frame)\n",
    "                if video_scale != 1:\n",
    "                    frame = pygame.transform.scale_by(frame, video_scale)\n",
    "                    \n",
    "                if screen_size[0] > shape[0]:\n",
    "                    screen.blit(frame, ( (screen_size[0] - shape[0])/2, video_y_pos))\n",
    "                else:\n",
    "                    screen.blit(frame, (0,0))\n",
    "\n",
    "                #display UI\n",
    "                screen.blit(micro_img, micro_pos)\n",
    "                screen.blit(sample_questions_img, sample_questions_img_pos)\n",
    "\n",
    "                # display text\n",
    "                if state == States.IDLE:\n",
    "                    screen.blit(space_key_text, space_key_text_pos)\n",
    "                elif state == States.LISTENING:\n",
    "                    if listening_thread and listening_thread.thinking:\n",
    "                        screen.blit(thinking_text, speak_text_pos)\n",
    "                    else:\n",
    "                        screen.blit(speak_text, speak_text_pos)\n",
    "                elif state == States.SPEAKING:\n",
    "                    if text_question:\n",
    "                        screen.blit(small_font.render(\"Your question:\" +text_question, True, text_question_color, (0,0,0)), text_question_pos)\n",
    "                    if text_response:\n",
    "                        text_reponse_shift += text_reponse_shift_speed\n",
    "                        screen.blit(small_font.render(text_response, True, text_response_color, (0,0,0)),\n",
    "                                    [text_response_pos[0]-text_reponse_shift, text_response_pos[1]])\n",
    "\n",
    "                if online_status:\n",
    "                    screen.blit(online_text_on, online_status_pos)\n",
    "                else:\n",
    "                    screen.blit(online_text_off, online_status_pos)\n",
    "                    \n",
    "                screen.blit(copyright_text, copyright_pos)\n",
    "                \n",
    "                # update\n",
    "                pygame.display.update()\n",
    "\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == KEYDOWN:\n",
    "                        if event.key == pygame.K_SPACE and state == States.IDLE:\n",
    "                            micro_img = micro_img_white\n",
    "                            state = States.LISTENING\n",
    "                            log(\"space\")\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(listen_video_file)\n",
    "\n",
    "                            listening_thread = VoiceRecordingThread()\n",
    "                            listening_thread.start()\n",
    "                            log(\"listening thread is started\")\n",
    "\n",
    "                        elif event.key == pygame.K_q:\n",
    "                            # exit\n",
    "                            #sys.exit(0)\n",
    "                            raise StopExecution\n",
    "                        elif event.key == pygame.K_i:\n",
    "                            online_status = not online_status\n",
    "                        elif event.key == pygame.K_b:\n",
    "                            # break and go to idle cycle\n",
    "                            if stop_event:\n",
    "                                stop_event.set()\n",
    "                            speaking_thread = None\n",
    "                            listening_thread = None\n",
    "                            state = States.IDLE\n",
    "                            cap.release()\n",
    "                            cap = cv2.VideoCapture(idle_video_file)\n",
    "                            #sys.exit(0)\n",
    "                    elif event.type == KEYUP:\n",
    "                        if event.key == pygame.K_SPACE:\n",
    "                            micro_img = micro_img_black\n",
    "                            if listening_thread:\n",
    "                                listening_thread.recognizer.energy_threshold = 10000\n",
    "                    elif event.type == pygame.QUIT:\n",
    "                        raise StopExecution\n",
    "                        #exit()\n",
    "                        #pygame.quit()\n",
    "                        #cv2.destroyAllWindows()\n",
    "\n",
    "                cv2.waitKey(25)\n",
    "                \n",
    "            else:\n",
    "                if state == States.IDLE:\n",
    "                    # random play hello\n",
    "                    if state == States.IDLE:\n",
    "                        if random() < 0.5 and not hello_played:\n",
    "                            hello_played = True\n",
    "                            rand_video_id = randint(0, len(special_videos)-1)\n",
    "                            cap = cv2.VideoCapture(special_videos[rand_video_id][0])\n",
    "                            mixer.init()\n",
    "                            mixer.music.load(special_videos[rand_video_id][1])\n",
    "                            mixer.music.play()\n",
    "                            \n",
    "                           # mixer.music.unload()\n",
    "                        else:\n",
    "                            hello_played = False\n",
    "                            cap = cv2.VideoCapture(idle_video_file)\n",
    "                    \n",
    "                elif state == States.LISTENING:\n",
    "                    cap = cv2.VideoCapture(listen_video_file)\n",
    "                else:\n",
    "                    cap = cv2.VideoCapture(speaking_video_file)\n",
    "                    \n",
    "    except (KeyboardInterrupt, SystemExit, StopExecution):\n",
    "        log(\"stopped\")\n",
    "        pygame.quit()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        \n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def shift_pitch(filename, higher_pitch_shift, lower_pitch_shift, load_original=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(filename)\n",
    "    if load_original:\n",
    "        IPython.display.display(Audio(data=audio_wavelength, rate=sampling_rate, autoplay=False))\n",
    "    if higher_pitch_shift > 0:\n",
    "        higher_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=higher_pitch_shift)\n",
    "        IPython.display.display(Audio(data=higher_pitch, rate=sampling_rate, autoplay=False))\n",
    "    if lower_pitch_shift < 0:\n",
    "        lower_pitch = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=lower_pitch_shift)\n",
    "        IPython.display.display(Audio(data=lower_pitch, rate=sampling_rate, autoplay=False))\n",
    "        \n",
    "def save_shifted_pitch(output_voice_file, output_voice_file_changed, pitch_shift, load_original=False, load_altered=False):\n",
    "    audio_wavelength, sampling_rate = librosa.load(output_voice_file)\n",
    "    altered = librosa.effects.pitch_shift(audio_wavelength, sr=sampling_rate, n_steps=pitch_shift)\n",
    "    sf.write(output_voice_file_changed, altered, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0cb7cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space\n",
      "listening thread is started\n",
      "speak\n",
      "User input: what is the weather in Frankfurt\n",
      "scores: 0.14327775 18\n",
      "offline, default response\n",
      "Answers/0401_response_unicorn.mp3\n",
      "Answers/0401_response_unicorn.mp4\n",
      "stopped\n",
      "audio end\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unicornVideo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca8c4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_prep(\"I'm sorry, I got disconnected from the Internet and I can't provide an answer to that question. Please feel free to reach out to the person at our exhibition stand for assistance. They will be happy to help with all your queries and provide you with more information about our products and services. Thank you for your understanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da302939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio end\n"
     ]
    }
   ],
   "source": [
    "PlaySoundThread(Event()).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d926a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.Microphone.list_microphone_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b27962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13aa0c5e",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
